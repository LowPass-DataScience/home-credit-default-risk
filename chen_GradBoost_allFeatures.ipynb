{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Use plotly offline for fancy plots\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "# use cufflinks to bind plotly to pandas\n",
    "import cufflinks as cf \n",
    "from os import listdir\n",
    "# for display control\n",
    "from IPython.display import display\n",
    "# Gradient boosting using LightBGM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Garbage collection\n",
    "import gc\n",
    "gc.enable()\n",
    "# Lock pseudo-number seed\n",
    "randSeed = 1\n",
    "np.random.seed(randSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global verbose control\n",
    "PREVIEW_DATASET = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to transform all catagorical fields using one hot ending\n",
    "def oneHotEncoding(df):\n",
    "    # Get list categorical features\n",
    "    catFeatures = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    # Convert to one hot encoding\n",
    "    ohe = pd.get_dummies(df, columns=catFeatures)\n",
    "    return ohe\n",
    "\n",
    "# Utility function to add descriptive statistics as secondary fields in the dataframe\n",
    "def addStatsFields(df, field):\n",
    "    df['MEAN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .mean()[field]\n",
    "    )\n",
    "    df['MEDIAN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .median()[field]\n",
    "    )\n",
    "    df['MAX_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .max()[field]\n",
    "    )\n",
    "    df['MIN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .min()[field]\n",
    "    )\n",
    "    df['SUM_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .sum()[field]\n",
    "    )\n",
    "    df['VAR_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .var()[field]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading application_train.csv.zip ...\n",
      "loading previous_application.csv.zip ...\n",
      "loading lowpass_submission_v1.csv ...\n",
      "loading bureau.csv.zip ...\n",
      "loading credit_card_balance.csv.zip ...\n",
      "loading application_test.csv.zip ...\n",
      "loading installments_payments.csv.zip ...\n",
      "loading bureau_balance.csv.zip ...\n",
      "loading POS_CASH_balance.csv.zip ...\n",
      "loading HomeCredit_columns_description.csv ...\n",
      "loading sample_submission.csv.zip ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "dataFiles = listdir(\"../data/home-credit-default-risk/\")\n",
    "for filename in dataFiles:\n",
    "    print(f'loading {filename} ...')\n",
    "    if '.csv.zip' in filename:\n",
    "        # compressed data file\n",
    "        locals()[filename.rstrip('.csv.zip')] = pd.read_csv(\n",
    "            f'../data/home-credit-default-risk/{filename}',\n",
    "            compression='zip', \n",
    "            header=0, \n",
    "            sep=',', \n",
    "            quotechar='\"'\n",
    "        )\n",
    "\n",
    "dataTrain = oneHotEncoding(application_train)\n",
    "dataTest = oneHotEncoding(application_test)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau datasets processed, 47 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preprocess bureau datasets\n",
    "if PREVIEW_DATASET:\n",
    "    print('Raw bureau_balance dataset')\n",
    "    display(bureau_balance.head(5))\n",
    "\n",
    "# Count by status\n",
    "bureauBalance = bureau_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts(normalize = False)\n",
    "# Pivot into table of status values\n",
    "bureauBalance = bureauBalance.unstack('STATUS')\n",
    "# Add months balance data as new fileds\n",
    "bureauBalance['MONTHS_COUNT'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU') \n",
    "        .MONTHS_BALANCE          \n",
    "        .size()\n",
    ")\n",
    "bureauBalance['MONTHS_MAX'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .MONTHS_BALANCE\n",
    "        .max()\n",
    ")\n",
    "bureauBalance['MONTHS_MIN'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .MONTHS_BALANCE\n",
    "        .min()\n",
    ")\n",
    "if PREVIEW_DATASET:\n",
    "    print('Formatted')\n",
    "    display(bureauBalance.head(5))\n",
    "\n",
    "# Finally, merge the two bureau table together \n",
    "bureauData = bureau.join(bureauBalance, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "# Transform features\n",
    "bureauData = oneHotEncoding(bureauData).groupby('SK_ID_CURR').mean()\n",
    "if PREVIEW_DATASET:\n",
    "    print('Merged and transformed')\n",
    "    display(bureauData.head(5))\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = bureauData,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = bureauData,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'bureau datasets processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables and clean up memory\n",
    "del bureauBalance\n",
    "del bureauData\n",
    "del bureau_balance\n",
    "del bureau\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_application datasets processed, 163 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces previous_application\n",
    "# Transform with one hot encoding\n",
    "prevApplication = oneHotEncoding(previous_application)\n",
    "\n",
    "# Compute number of previous applications by counting SK_ID_PREV\n",
    "prevApplicationCnt = (\n",
    "    prevApplication[['SK_ID_CURR', 'SK_ID_PREV']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .count()\n",
    ")\n",
    "# Remove the SK_ID_PREV feature because the average of it is meaningless\n",
    "del prevApplication['SK_ID_PREV']\n",
    "# Group by mean\n",
    "prevApplication = prevApplication.groupby('SK_ID_CURR').mean()\n",
    "# Add back the number of previous applications\n",
    "prevApplication['PREV_APPLICATION_CNT'] = prevApplicationCnt['SK_ID_PREV']\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Merged dataset')\n",
    "    display(prevApplication.head(5))\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = prevApplication,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = prevApplication,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'previous_application datasets processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables and clean up memory\n",
    "del prevApplicationCnt\n",
    "del prevApplication\n",
    "del previous_application\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_CASH_balance dataset processed, 38 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces POS_CASH_balance\n",
    "# Transform with one hot encoding\n",
    "posCashBal = oneHotEncoding(POS_CASH_balance)\n",
    "\n",
    "# Add some secondary features\n",
    "posCashBal = addStatsFields(posCashBal, 'SK_DPD')\n",
    "posCashBal = addStatsFields(posCashBal, 'SK_DPD_DEF')\n",
    "posCashBal = addStatsFields(posCashBal, 'CNT_INSTALMENT_FUTURE')\n",
    "posCashBal = addStatsFields(posCashBal, 'CNT_INSTALMENT')\n",
    "\n",
    "# Group by UserID\n",
    "del posCashBal['SK_ID_PREV']\n",
    "posCashBal = posCashBal.groupby('SK_ID_CURR').mean()\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Processed dataset')\n",
    "    display(posCashBal.head())\n",
    "    \n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = posCashBal,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = posCashBal,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'POS_CASH_balance dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del posCashBal\n",
    "del POS_CASH_balance\n",
    "gc.collect();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_CASH_balance dataset processed, 129 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces credit_card_balance\n",
    "# Transform with one hot encoding\n",
    "# Rename fields that have already used in POS_CASH_balance\n",
    "creditCardBal = credit_card_balance.rename(\n",
    "    index = str, \n",
    "    columns = {\n",
    "        'NAME_CONTRACT_STATUS' : 'NAME_CONTRACT_STATUS_CREDIT',\n",
    "        'SK_DPD' : 'SK_DPD_CREDIT',\n",
    "        'SK_DPD_DEF' : 'SK_DPD_DEFCREDIT'\n",
    "    }\n",
    ")\n",
    "creditCardBal = oneHotEncoding(creditCardBal)\n",
    "\n",
    "# Add some secondary features\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_BALANCE')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_CREDIT_LIMIT_ACTUAL')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_ATM_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_OTHER_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_POS_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_INST_MIN_REGULARITY')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_PAYMENT_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_PAYMENT_TOTAL_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_RECEIVABLE_PRINCIPAL')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_RECIVABLE')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'AMT_TOTAL_RECEIVABLE')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_ATM_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_OTHER_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_POS_CURRENT')\n",
    "creditCardBal = addStatsFields(creditCardBal, 'CNT_INSTALMENT_MATURE_CUM')\n",
    "\n",
    "# Group by UserID\n",
    "del creditCardBal['SK_ID_PREV']\n",
    "creditCardBal = creditCardBal.groupby('SK_ID_CURR').mean()\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Processed dataset')\n",
    "    display(creditCardBal.head())\n",
    "    \n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = creditCardBal,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = creditCardBal,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'credit_card_balance dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del creditCardBal\n",
    "del credit_card_balance\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installments_payment dataset processed, 30 new features added\n"
     ]
    }
   ],
   "source": [
    "colDictMean = {}\n",
    "colDictMin = {}\n",
    "colDictMax = {}\n",
    "colDictMedian = {}\n",
    "colDictVar = {}\n",
    "for col in installments_payment.columns:\n",
    "    if col not in ['SK_ID_CURR', 'SK_ID_PREV']:\n",
    "        colDictMean[col] = 'MEAN_' + col\n",
    "        colDictMin[col] = 'MIN_' + col\n",
    "        colDictMax[col] = 'MAX_' + col\n",
    "        colDictMedian[col] = 'MEDIAN_' + col\n",
    "        colDictVar[col] = 'VAR_' + col\n",
    "\n",
    "# Add mean\n",
    "installPayment = installments_payment.groupby('SK_ID_CURR').mean()\n",
    "del installPayment['SK_ID_PREV']\n",
    "installPayment = installPayment.rename(\n",
    "    columns = colDictMean\n",
    ")\n",
    "# Add min\n",
    "installPaymentMin = installments_payment.groupby('SK_ID_CURR').min()\n",
    "del installPaymentMin['SK_ID_PREV']\n",
    "installPaymentMin = installPaymentMin.rename(\n",
    "    columns = colDictMin\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMin,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMin\n",
    "# Add max\n",
    "installPaymentMax = installments_payment.groupby('SK_ID_CURR').max()\n",
    "del installPaymentMax['SK_ID_PREV']\n",
    "installPaymentMax = installPaymentMax.rename(\n",
    "    columns = colDictMax\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMax,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMax\n",
    "# Add median\n",
    "installPaymentMedian = installments_payment.groupby('SK_ID_CURR').median()\n",
    "del installPaymentMedian['SK_ID_PREV']\n",
    "installPaymentMedian = installPaymentMedian.rename(\n",
    "    columns = colDictMedian\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMedian,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMedian\n",
    "# Add variance\n",
    "installPaymentVar = installments_payment.groupby('SK_ID_CURR').var()\n",
    "del installPaymentVar['SK_ID_PREV']\n",
    "installPaymentVar = installPaymentVar.rename(\n",
    "    columns = colDictVar\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentVar,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentVar\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = installPayment,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = installPayment,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'installments_payment dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del installPayment\n",
    "del installments_payment\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>CNT_INSTALMENT</th>\n",
       "      <th>CNT_INSTALMENT_FUTURE</th>\n",
       "      <th>NAME_CONTRACT_STATUS</th>\n",
       "      <th>SK_DPD</th>\n",
       "      <th>SK_DPD_DEF</th>\n",
       "      <th>NUM_UNIQUE_STATUS</th>\n",
       "      <th>NUM_MAX_STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1803195</td>\n",
       "      <td>182943</td>\n",
       "      <td>-31</td>\n",
       "      <td>48.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1715348</td>\n",
       "      <td>367990</td>\n",
       "      <td>-33</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1784872</td>\n",
       "      <td>397406</td>\n",
       "      <td>-32</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1903291</td>\n",
       "      <td>269225</td>\n",
       "      <td>-35</td>\n",
       "      <td>48.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2341044</td>\n",
       "      <td>334279</td>\n",
       "      <td>-35</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV  SK_ID_CURR  MONTHS_BALANCE  CNT_INSTALMENT  \\\n",
       "0     1803195      182943             -31            48.0   \n",
       "1     1715348      367990             -33            36.0   \n",
       "2     1784872      397406             -32            12.0   \n",
       "3     1903291      269225             -35            48.0   \n",
       "4     2341044      334279             -35            36.0   \n",
       "\n",
       "   CNT_INSTALMENT_FUTURE  NAME_CONTRACT_STATUS  SK_DPD  SK_DPD_DEF  \\\n",
       "0                   45.0                     0       0           0   \n",
       "1                   35.0                     0       0           0   \n",
       "2                    9.0                     0       0           0   \n",
       "3                   42.0                     0       0           0   \n",
       "4                   35.0                     0       0           0   \n",
       "\n",
       "   NUM_UNIQUE_STATUS  NUM_MAX_STATUS  \n",
       "0                NaN             NaN  \n",
       "1                NaN             NaN  \n",
       "2                NaN             NaN  \n",
       "3                NaN             NaN  \n",
       "4                NaN             NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preproces POS_CASH_balance\n",
    "# Transform with one hot encoding\n",
    "#posCashBal = oneHotEncoding(POS_CASH_balance)\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "#display(POS_CASH_balance.head(5))\n",
    "#oneHotEncoding(POS_CASH_balance).head(5)\n",
    "#POS_CASH_balance['NAME_CONTRACT_STATUS'].astype(str)\n",
    "posCashBal = POS_CASH_balance\n",
    "le = LabelEncoder()\n",
    "posCashBal.NAME_CONTRACT_STATUS = le.fit_transform(posCashBal['NAME_CONTRACT_STATUS'].astype(str))\n",
    "posCashBal['NUM_UNIQUE_STATUS'] = (\n",
    "    posCashBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .nunique()\n",
    "        .NAME_CONTRACT_STATUS\n",
    ")\n",
    "posCashBal['NUM_MAX_STATUS'] = (\n",
    "    posCashBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .max()\n",
    "        .NAME_CONTRACT_STATUS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has 307511 samples, and 652 features\n"
     ]
    }
   ],
   "source": [
    "ignoredFields = ['TARGET']\n",
    "# Assemble into I/O dataset format\n",
    "#   X - All fields other than 'TARGET'\n",
    "#   Y - 'TARGET' fields\n",
    "dataCols = dataTrain.columns\n",
    "trainDataMask = [col for col in dataCols if col not in ignoredFields]\n",
    "x = dataTrain[trainDataMask]\n",
    "y = dataTrain['TARGET']\n",
    "\n",
    "# Summarize training dataset\n",
    "featureCnt = len(dataTrain.keys()) - 1\n",
    "numSamples = len(dataTrain)\n",
    "print(f'Training dataset has {numSamples} samples, and {featureCnt} features')\n",
    "\n",
    "# Split training data randomly using train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, \n",
    "    test_size = 0.10,\n",
    "    random_state = randSeed\n",
    ")\n",
    "\n",
    "# Create lgb dataset\n",
    "lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "lgb_test = lgb.Dataset(data=x_test, label=y_test)\n",
    "\n",
    "# Free up memory\n",
    "del x\n",
    "del y\n",
    "del dataCols\n",
    "del trainDataMask\n",
    "del dataTrain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning:\n",
      "\n",
      "Found `num_iteration` in params. Will use it instead of argument\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's auc: 0.758439\n",
      "[100]\tvalid_0's auc: 0.773542\n",
      "[150]\tvalid_0's auc: 0.782224\n",
      "[200]\tvalid_0's auc: 0.78665\n",
      "[250]\tvalid_0's auc: 0.78883\n",
      "[300]\tvalid_0's auc: 0.790031\n",
      "[350]\tvalid_0's auc: 0.790758\n",
      "[400]\tvalid_0's auc: 0.790981\n",
      "[450]\tvalid_0's auc: 0.791342\n",
      "[500]\tvalid_0's auc: 0.791474\n",
      "[550]\tvalid_0's auc: 0.791472\n",
      "Early stopping, best iteration is:\n",
      "[532]\tvalid_0's auc: 0.791658\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'device' : 'cpu',\n",
    "    'nthread': 8,            # [CPU] number of OpenMP threads\n",
    "    'gpu_use_dp' : 'false',  # [GPU] set to 1 to enable 64bit float point\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'num_iteration': 1000,\n",
    "    'num_leaves': 32,\n",
    "    'metric': 'auc',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 10,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_bin': 256,\n",
    "    'max_depth' : 10,\n",
    "    'min_data_in_leaf': 32,\n",
    "    'min_split_gain': 0.5,\n",
    "    'min_child_weight': 1,\n",
    "    'min_child_samples': 5,\n",
    "    'subsample_for_bin': 200,\n",
    "    'subsample': 1,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round = 50,\n",
    "    valid_sets = lgb_test,\n",
    "    early_stopping_rounds = 50,\n",
    "    verbose_eval = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and save to csv\n",
    "predResult = gbm.predict(dataTest)\n",
    "submissionDataset = sample_submission\n",
    "submissionDataset.TARGET = predResult\n",
    "submissionDataset.to_csv('./lowpass_submission_v2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
