{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Use plotly offline for fancy plots\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "# use cufflinks to bind plotly to pandas\n",
    "import cufflinks as cf \n",
    "from os import listdir\n",
    "# for display control\n",
    "from IPython.display import display\n",
    "# Gradient boosting using LightBGM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Use LabelEncoder that seems to yield better result\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# For parallel multi-threading\n",
    "from multiprocessing import Pool, cpu_count, Array\n",
    "# Garbage collection\n",
    "import gc\n",
    "gc.enable()\n",
    "# Lock pseudo-number seed\n",
    "randSeed = 1\n",
    "np.random.seed(randSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global verbose control\n",
    "PREVIEW_DATASET = 0\n",
    "ADD_STATS_FEATURES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to transform all catagorical fields using one hot ending\n",
    "def oneHotEncoding(df):\n",
    "    # Get list categorical features\n",
    "    catFeatures = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    # Convert to one hot encoding\n",
    "    ohe = pd.get_dummies(df, columns=catFeatures)\n",
    "    return ohe\n",
    "\n",
    "# Utility function to encode catagorical columns using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "def encodeLabel(field):\n",
    "    return le.fit_transform(field.astype(str))\n",
    "\n",
    "# Utility function to add descriptive statistics as secondary fields in the dataframe\n",
    "def addStatsFields(df, field):\n",
    "    df['MEAN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .mean()[field]\n",
    "    )\n",
    "    df['MEDIAN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .median()[field]\n",
    "    )\n",
    "    df['MAX_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .max()[field]\n",
    "    )\n",
    "    df['MIN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .min()[field]\n",
    "    )\n",
    "    df['SUM_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .sum()[field]\n",
    "    )\n",
    "    df['VAR_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .var()[field]\n",
    "    )\n",
    "    df['CNT_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .count()[field]\n",
    "    )\n",
    "    # Drop current field\n",
    "    df.drop(\n",
    "        [field], \n",
    "        axis = 1, \n",
    "        inplace = True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bureau_balance.csv.zip ...\n",
      "loading HomeCredit_columns_description.csv ...\n",
      "loading installments_payments.csv.zip ...\n",
      "loading previous_application.csv.zip ...\n",
      "loading application_test.csv.zip ...\n",
      "loading POS_CASH_balance.csv.zip ...\n",
      "loading credit_card_balance.csv.zip ...\n",
      "loading application_train.csv.zip ...\n",
      "loading bureau.csv.zip ...\n",
      "loading sample_submission.csv.zip ...\n",
      "Training dataset has 307511 samples, and 244 features\n",
      "Testing dataset has 48744 samples, and 244 features\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "dataPath = '/media/ephemeral0/data/home-credit-default-risk/'\n",
    "dataFiles = listdir(f\"{dataPath}\")\n",
    "for filename in dataFiles:\n",
    "    print(f'loading {filename} ...')\n",
    "    if '.csv.zip' in filename:\n",
    "        # compressed data file\n",
    "        locals()[filename.rstrip('.csv.zip')] = pd.read_csv(\n",
    "            f'{dataPath}/{filename}',\n",
    "            compression='zip', \n",
    "            header=0, \n",
    "            sep=',', \n",
    "            quotechar='\"'\n",
    "        )\n",
    "\n",
    "# Get output label and remove it from feature list\n",
    "dataTrain = application_train\n",
    "dataTest = application_test\n",
    "y = dataTrain['TARGET']\n",
    "\n",
    "# Transform using One Hot Encoding \n",
    "# (using only the training dataset features as reference)\n",
    "catFeatures = [\n",
    "    col \n",
    "    for col in dataTrain.columns \n",
    "    if dataTrain[col].dtype == 'object'\n",
    "]\n",
    "ohe = pd.concat([dataTrain,dataTest], sort=False)\n",
    "ohe = pd.get_dummies(ohe, columns = catFeatures)\n",
    "dataTrain = ohe.iloc[:dataTrain.shape[0],:]\n",
    "dataTest = ohe.iloc[dataTrain.shape[0]:,]\n",
    "del dataTrain['TARGET']\n",
    "del dataTest['TARGET']\n",
    "\n",
    "# Summarize dataset\n",
    "featureCnt = len(dataTrain.keys()) - 1\n",
    "numSamples = len(dataTrain)\n",
    "print(f'Training dataset has {numSamples} samples, and {featureCnt} features')\n",
    "featureCnt = len(dataTest.keys()) - 1\n",
    "numSamples = len(dataTest)\n",
    "print(f'Testing dataset has {numSamples} samples, and {featureCnt} features')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau datasets processed, 47 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preprocess bureau datasets\n",
    "if PREVIEW_DATASET:\n",
    "    print('Raw bureau_balance dataset')\n",
    "    display(bureau_balance.head(5))\n",
    "\n",
    "# Count by status\n",
    "bureauBalance = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .STATUS\n",
    "        .value_counts(normalize = False)\n",
    "        .unstack('STATUS')\n",
    ")\n",
    "# Rename columns to avoid conflict\n",
    "renameDict = {}\n",
    "for col in bureauBalance.columns:\n",
    "    renameDict[col] = 'STATUS_' + col\n",
    "bureauBalance.rename(columns = renameDict)\n",
    "\n",
    "# Add months balance data as new features\n",
    "bureauBalance['MONTHS_COUNT'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU') \n",
    "        .MONTHS_BALANCE          \n",
    "        .size()\n",
    ")\n",
    "bureauBalance['MONTHS_MAX'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .MONTHS_BALANCE\n",
    "        .max()\n",
    ")\n",
    "bureauBalance['MONTHS_MIN'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .MONTHS_BALANCE\n",
    "        .min()\n",
    ")\n",
    "if PREVIEW_DATASET:\n",
    "    print('Formatted')\n",
    "    display(bureauBalance.head(5))\n",
    "\n",
    "# Finally, merge the two bureau table together \n",
    "bureauData = bureau.join(bureauBalance, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "# Transform features\n",
    "bureauData = oneHotEncoding(bureauData).groupby('SK_ID_CURR').mean()\n",
    "bureauData['CNT_BURO'] = (\n",
    "    bureau[['SK_ID_BUREAU', 'SK_ID_CURR']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .count()['SK_ID_BUREAU']\n",
    ")\n",
    "del bureauData['SK_ID_BUREAU']\n",
    "if PREVIEW_DATASET:\n",
    "    print('Merged and transformed')\n",
    "    display(bureauData.head(5))\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = bureauData.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = bureauData.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'bureau datasets processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables and clean up memory\n",
    "del bureauBalance\n",
    "del bureauData\n",
    "del bureau_balance\n",
    "del bureau\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_application datasets processed, 163 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces previous_application\n",
    "# Transform with one hot encoding\n",
    "prevApplication = oneHotEncoding(previous_application)\n",
    "\n",
    "# Compute number of previous applications by counting SK_ID_PREV\n",
    "prevApplicationCnt = (\n",
    "    prevApplication[['SK_ID_CURR', 'SK_ID_PREV']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .count()\n",
    "        .SK_ID_PREV\n",
    ")\n",
    "# Group by mean\n",
    "prevApplication = prevApplication.groupby('SK_ID_CURR').mean()\n",
    "# Add back the number of previous applications\n",
    "prevApplication['CNT_PREV_APPLICATION'] = prevApplicationCnt\n",
    "# Remove the SK_ID_PREV feature because the average of it is meaningless\n",
    "del prevApplication['SK_ID_PREV']\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Merged dataset')\n",
    "    display(prevApplication.head(5))\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = prevApplication.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = prevApplication.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'previous_application datasets processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables and clean up memory\n",
    "del prevApplicationCnt\n",
    "del prevApplication\n",
    "del previous_application\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_CASH_balance dataset processed, 31 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces POS_CASH_balance\n",
    "# Encode using LabelEncoder\n",
    "posCashBal = POS_CASH_balance\n",
    "posCashBal.NAME_CONTRACT_STATUS = encodeLabel(POS_CASH_balance.NAME_CONTRACT_STATUS)\n",
    "posCashBal['CNT_UNIQUE_STATUS'] = (\n",
    "    posCashBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .nunique()\n",
    "        .NAME_CONTRACT_STATUS\n",
    ")\n",
    "posCashBal['MAX_UNIQUE_STATUS'] = (\n",
    "    posCashBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .max()\n",
    "        .NAME_CONTRACT_STATUS\n",
    ")\n",
    "\n",
    "# Add some secondary features\n",
    "if ADD_STATS_FEATURES:\n",
    "    posCashBal = addStatsFields(posCashBal, 'SK_DPD')\n",
    "    posCashBal = addStatsFields(posCashBal, 'SK_DPD_DEF')\n",
    "    posCashBal = addStatsFields(posCashBal, 'CNT_INSTALMENT_FUTURE')\n",
    "    posCashBal = addStatsFields(posCashBal, 'CNT_INSTALMENT')\n",
    "\n",
    "# Group by UserID\n",
    "#del posCashBal['SK_ID_PREV']\n",
    "posCashBal.drop(\n",
    "    ['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], \n",
    "    axis = 1, \n",
    "    inplace = True\n",
    ")\n",
    "posCashBal = posCashBal.groupby('SK_ID_CURR').mean()\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Processed dataset')\n",
    "    display(posCashBal.head())\n",
    "    \n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = posCashBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = posCashBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'POS_CASH_balance dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del posCashBal\n",
    "del POS_CASH_balance\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_card_balance dataset processed, 136 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces credit_card_balance\n",
    "# Transform with one hot encoding\n",
    "# Rename fields that have already used in POS_CASH_balance\n",
    "creditCardBal = credit_card_balance.rename(\n",
    "    index = str, \n",
    "    columns = {\n",
    "        'NAME_CONTRACT_STATUS' : 'NAME_CONTRACT_STATUS_CREDIT',\n",
    "        'SK_DPD' : 'SK_DPD_CREDIT',\n",
    "        'SK_DPD_DEF' : 'SK_DPD_DEF_CREDIT'\n",
    "    }\n",
    ")\n",
    "#creditCardBal = oneHotEncoding(creditCardBal)\n",
    "creditCardBal.NAME_CONTRACT_STATUS_CREDIT = encodeLabel(\n",
    "    creditCardBal.NAME_CONTRACT_STATUS_CREDIT\n",
    ")\n",
    "creditCardBal['CNT_UNIQUE_STATUS_CREDIT'] = (\n",
    "    creditCardBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS_CREDIT']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .nunique()\n",
    "        .NAME_CONTRACT_STATUS_CREDIT\n",
    ")\n",
    "creditCardBal['MAX_UNIQUE_STATUS_CREDIT'] = (\n",
    "    creditCardBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS_CREDIT']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .max()\n",
    "        .NAME_CONTRACT_STATUS_CREDIT\n",
    ")\n",
    "# Add some secondary features\n",
    "if ADD_STATS_FEATURES:\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_BALANCE')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_CREDIT_LIMIT_ACTUAL')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_ATM_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_OTHER_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_POS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_INST_MIN_REGULARITY')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_PAYMENT_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_PAYMENT_TOTAL_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_RECEIVABLE_PRINCIPAL')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_RECIVABLE')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_TOTAL_RECEIVABLE')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_ATM_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_OTHER_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_POS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_INSTALMENT_MATURE_CUM')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'SK_DPD_CREDIT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'SK_DPD_DEF_CREDIT')\n",
    "\n",
    "# Group by ID\n",
    "creditCardBal.drop(\n",
    "    ['SK_ID_PREV', 'NAME_CONTRACT_STATUS_CREDIT'], \n",
    "    axis = 1, \n",
    "    inplace = True\n",
    ")\n",
    "creditCardBal = creditCardBal.groupby('SK_ID_CURR').mean()\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Processed dataset')\n",
    "    display(creditCardBal.head())\n",
    "    \n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = creditCardBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = creditCardBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'credit_card_balance dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del creditCardBal\n",
    "del credit_card_balance\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installments_payment dataset processed, 30 new features added\n"
     ]
    }
   ],
   "source": [
    "colDictMean = {}\n",
    "colDictMin = {}\n",
    "colDictMax = {}\n",
    "colDictMedian = {}\n",
    "colDictVar = {}\n",
    "for col in installments_payment.columns:\n",
    "    if col not in ['SK_ID_CURR', 'SK_ID_PREV']:\n",
    "        colDictMean[col] = 'MEAN_' + col\n",
    "        colDictMin[col] = 'MIN_' + col\n",
    "        colDictMax[col] = 'MAX_' + col\n",
    "        colDictMedian[col] = 'MEDIAN_' + col\n",
    "        colDictVar[col] = 'VAR_' + col\n",
    "\n",
    "# Add mean\n",
    "installPayment = installments_payment.groupby('SK_ID_CURR').mean()\n",
    "del installPayment['SK_ID_PREV']\n",
    "installPayment = installPayment.rename(\n",
    "    columns = colDictMean\n",
    ")\n",
    "# Add min\n",
    "installPaymentMin = installments_payment.groupby('SK_ID_CURR').min()\n",
    "del installPaymentMin['SK_ID_PREV']\n",
    "installPaymentMin = installPaymentMin.rename(\n",
    "    columns = colDictMin\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMin,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMin\n",
    "# Add max\n",
    "installPaymentMax = installments_payment.groupby('SK_ID_CURR').max()\n",
    "del installPaymentMax['SK_ID_PREV']\n",
    "installPaymentMax = installPaymentMax.rename(\n",
    "    columns = colDictMax\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMax,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMax\n",
    "# Add median\n",
    "installPaymentMedian = installments_payment.groupby('SK_ID_CURR').median()\n",
    "del installPaymentMedian['SK_ID_PREV']\n",
    "installPaymentMedian = installPaymentMedian.rename(\n",
    "    columns = colDictMedian\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMedian,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMedian\n",
    "# Add variance\n",
    "installPaymentVar = installments_payment.groupby('SK_ID_CURR').var()\n",
    "del installPaymentVar['SK_ID_PREV']\n",
    "installPaymentVar = installPaymentVar.rename(\n",
    "    columns = colDictVar\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentVar,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentVar\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = installPayment.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = installPayment.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'installments_payment dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del installPayment\n",
    "del installments_payment\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has 307511 samples, and 508 features\n",
      "Testing dataset has 48744 samples, and 508 features\n"
     ]
    }
   ],
   "source": [
    "## Final processing\n",
    "# Remove features too many missing values\n",
    "dataTestFinal  = dataTest[dataTest.columns[dataTrain.isnull().mean() < 0.85]]\n",
    "dataTrainFinal = dataTrain[dataTrain.columns[dataTrain.isnull().mean() < 0.85]]\n",
    "\n",
    "# Delete SK_ID_CURR field (not a feature) \n",
    "del dataTestFinal['SK_ID_CURR']\n",
    "del dataTrainFinal['SK_ID_CURR']\n",
    "\n",
    "# Summarize dataset\n",
    "featureCnt = len(dataTrainFinal.keys()) - 1\n",
    "numSamples = len(dataTrainFinal)\n",
    "print(f'Training dataset has {numSamples} samples, and {featureCnt} features')\n",
    "featureCnt = len(dataTestFinal.keys()) - 1\n",
    "numSamples = len(dataTestFinal)\n",
    "print(f'Testing dataset has {numSamples} samples, and {featureCnt} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data randomly using train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataTrainFinal, y, \n",
    "    test_size = 0.15,\n",
    "    random_state = randSeed,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# Create lgb dataset\n",
    "lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "lgb_test = lgb.Dataset(data=x_test, label=y_test)\n",
    "\n",
    "# Free up memory\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning and Performance References\n",
    "1. [LightGBM - Parameters](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst)\n",
    "2. [LightGBM - \n",
    "Parallel Learning Guide](https://github.com/Microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst)\n",
    "3. [LightGBM - Parameters Tuning Guide](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst)\n",
    "4. [Kaggle - GBM vs. XGBoost vs. LightGBM](https://www.kaggle.com/nschneider/gbm-vs-xgboost-vs-lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[25]\tvalid_0's auc: 0.747366\n",
      "[50]\tvalid_0's auc: 0.757548\n",
      "[75]\tvalid_0's auc: 0.768338\n",
      "[100]\tvalid_0's auc: 0.777192\n",
      "[125]\tvalid_0's auc: 0.782582\n",
      "[150]\tvalid_0's auc: 0.785385\n",
      "[175]\tvalid_0's auc: 0.78696\n",
      "[200]\tvalid_0's auc: 0.788181\n",
      "[225]\tvalid_0's auc: 0.789046\n",
      "[250]\tvalid_0's auc: 0.789768\n",
      "[275]\tvalid_0's auc: 0.790552\n",
      "[300]\tvalid_0's auc: 0.790796\n",
      "[325]\tvalid_0's auc: 0.790991\n",
      "[350]\tvalid_0's auc: 0.79109\n",
      "[375]\tvalid_0's auc: 0.79094\n",
      "[400]\tvalid_0's auc: 0.791312\n",
      "[425]\tvalid_0's auc: 0.791466\n",
      "[450]\tvalid_0's auc: 0.791721\n",
      "[475]\tvalid_0's auc: 0.791994\n",
      "[500]\tvalid_0's auc: 0.792069\n",
      "[525]\tvalid_0's auc: 0.792007\n",
      "[550]\tvalid_0's auc: 0.791727\n",
      "Early stopping, best iteration is:\n",
      "[507]\tvalid_0's auc: 0.792155\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'device' : 'cpu',\n",
    "    'nthread': 8,            # [CPU] number of OpenMP threads\n",
    "    'tree_learner' : 'feature',\n",
    "    'gpu_use_dp' : 'false',  # [GPU] set to 1 to enable 64bit float point\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'auc',\n",
    "    'reg_alpha': 2,\n",
    "    'reg_lambda': 4,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_bin': 384,\n",
    "    'max_depth' : 11,\n",
    "    'min_split_gain': 0.5,\n",
    "    'min_child_weight': 1e-0,\n",
    "    'min_child_samples': 2,\n",
    "    'subsample_for_bin': 200000,\n",
    "    'subsample': 1,\n",
    "    'subsample_freq': 2,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round = 5000,\n",
    "    valid_sets = lgb_test,\n",
    "    early_stopping_rounds = 50,\n",
    "    verbose_eval = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and save to csv\n",
    "predResult = gbm.predict(dataTestFinal)\n",
    "submissionDataset = sample_submission\n",
    "submissionDataset.TARGET = predResult\n",
    "submissionDataset.to_csv('./lowpass_submission_v2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research zone for parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 0 - Cross-validation using AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new simulation thread (1/6), progress 4.17% (1/24)\n",
      "Starting new simulation thread (2/6), progress 8.33% (2/24)\n",
      "Starting new simulation thread (3/6), progress 12.50% (3/24)\n",
      "Starting new simulation thread (4/6), progress 16.67% (4/24)\n",
      "Starting new simulation thread (5/6), progress 20.83% (5/24)\n",
      "Starting new simulation thread (6/6), progress 25.00% (6/24)\n",
      "All thread used, waiting until finish... ...\n",
      "Simulation Batch done!\n",
      "Starting new simulation thread (1/6), progress 29.17% (7/24)\n",
      "Starting new simulation thread (2/6), progress 33.33% (8/24)\n",
      "Starting new simulation thread (3/6), progress 37.50% (9/24)\n",
      "Starting new simulation thread (4/6), progress 41.67% (10/24)\n",
      "Starting new simulation thread (5/6), progress 45.83% (11/24)\n",
      "Starting new simulation thread (6/6), progress 50.00% (12/24)\n",
      "All thread used, waiting until finish... ...\n",
      "Simulation Batch done!\n",
      "Starting new simulation thread (1/6), progress 54.17% (13/24)\n",
      "Starting new simulation thread (2/6), progress 58.33% (14/24)\n",
      "Starting new simulation thread (3/6), progress 62.50% (15/24)\n",
      "Starting new simulation thread (4/6), progress 66.67% (16/24)\n",
      "Starting new simulation thread (5/6), progress 70.83% (17/24)\n",
      "Starting new simulation thread (6/6), progress 75.00% (18/24)\n",
      "All thread used, waiting until finish... ...\n",
      "Simulation Batch done!\n",
      "Starting new simulation thread (1/6), progress 79.17% (19/24)\n",
      "Starting new simulation thread (2/6), progress 83.33% (20/24)\n",
      "Starting new simulation thread (3/6), progress 87.50% (21/24)\n",
      "Starting new simulation thread (4/6), progress 91.67% (22/24)\n",
      "Starting new simulation thread (5/6), progress 95.83% (23/24)\n",
      "Starting new simulation thread (6/6), progress 100.00% (24/24)\n",
      "Waiting for remaining jobs to finish... ...\n",
      "Simulation done!\n"
     ]
    }
   ],
   "source": [
    "gc.enable()\n",
    "gc.collect()\n",
    "nCVSamps = 24\n",
    "nThread = min(nCVSamps, 6) # cpu_count()\n",
    "# Cross validation\n",
    "def crossValidate(seed, cvData, idx):\n",
    "    # Generate new set of test dataset\n",
    "    _, x_test, _, y_test = train_test_split(\n",
    "        dataTrainFinal, y, \n",
    "        test_size = 0.50,\n",
    "        random_state = seed,\n",
    "        shuffle = True\n",
    "    )\n",
    "    predResult = gbm.predict(x_test)\n",
    "    cvData[idx] = roc_auc_score(y_true=y_test, y_score=predResult)\n",
    "\n",
    "# The following code submits parallel jobs using multiprocessing \n",
    "# to speed up the execution\n",
    "cvData = Array('f', np.empty([nCVSamps]))\n",
    "pool = Pool(processes=nThread)\n",
    "jobs = []\n",
    "for it in range(nCVSamps):\n",
    "    if len(jobs) == nThread:\n",
    "        print(\"All thread used, waiting until finish... ...\")\n",
    "        # Wait until all the active thread to finish\n",
    "        for job in jobs:\n",
    "            job.join()\n",
    "        print(\"Simulation Batch done!\")\n",
    "        # Clear the pool\n",
    "        jobs = []\n",
    "\n",
    "    # Start a new job thread\n",
    "    p = pool.Process(target=crossValidate, args=(it, cvData, it))\n",
    "    p.start()\n",
    "    jobs.append(p)\n",
    "\n",
    "    print(\"Starting new simulation thread ({0:d}/{1:d}), progress {2:.2f}% ({3:d}/{4:d})\".format(\n",
    "        len(jobs), nThread,\n",
    "        100.0*(it+1)/nCVSamps, it+1, nCVSamps))\n",
    "\n",
    "print(\"Waiting for remaining jobs to finish... ...\")\n",
    "# Wait until all the active thread to finish\n",
    "for job in jobs:\n",
    "    job.join()\n",
    "# Retreve data from shared memory\n",
    "cvData = np.array(cvData[:])\n",
    "# Clean up\n",
    "pool.close()\n",
    "gc.collect()\n",
    "print(\"Simulation done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "lines+markers",
         "type": "scatter",
         "y": [
          0.8942621350288391,
          0.8820658326148987,
          0.8934822678565979,
          0.8923944234848022,
          0.8937168121337891,
          0.8944800496101379,
          0.8939221501350403,
          0.893328070640564,
          0.8927224278450012,
          0.893109917640686,
          0.8928244113922119,
          0.8919203877449036,
          0.8938892483711243,
          0.8925439715385437,
          0.8926624059677124,
          0.8922682404518127,
          0.8945183753967285,
          0.8932705521583557,
          0.8930544853210449,
          0.8929429054260254,
          0.8936810493469238,
          0.8938148617744446,
          0.8921317458152771,
          0.8940614461898804
         ]
        }
       ],
       "layout": {
        "title": "ROC AUC value by rand number seed",
        "xaxis": {
         "title": "Random number seed"
        },
        "yaxis": {
         "title": "AUC"
        }
       }
      },
      "text/html": [
       "<div id=\"f2ef6d8d-fc07-4835-af53-fc9cf832c4cc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f2ef6d8d-fc07-4835-af53-fc9cf832c4cc\", [{\"type\": \"scatter\", \"y\": [0.8942621350288391, 0.8820658326148987, 0.8934822678565979, 0.8923944234848022, 0.8937168121337891, 0.8944800496101379, 0.8939221501350403, 0.893328070640564, 0.8927224278450012, 0.893109917640686, 0.8928244113922119, 0.8919203877449036, 0.8938892483711243, 0.8925439715385437, 0.8926624059677124, 0.8922682404518127, 0.8945183753967285, 0.8932705521583557, 0.8930544853210449, 0.8929429054260254, 0.8936810493469238, 0.8938148617744446, 0.8921317458152771, 0.8940614461898804], \"mode\": \"lines+markers\", \"name\": \"lines+markers\"}], {\"title\": \"ROC AUC value by rand number seed\", \"xaxis\": {\"title\": \"Random number seed\"}, \"yaxis\": {\"title\": \"AUC\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"f2ef6d8d-fc07-4835-af53-fc9cf832c4cc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f2ef6d8d-fc07-4835-af53-fc9cf832c4cc\", [{\"type\": \"scatter\", \"y\": [0.8942621350288391, 0.8820658326148987, 0.8934822678565979, 0.8923944234848022, 0.8937168121337891, 0.8944800496101379, 0.8939221501350403, 0.893328070640564, 0.8927224278450012, 0.893109917640686, 0.8928244113922119, 0.8919203877449036, 0.8938892483711243, 0.8925439715385437, 0.8926624059677124, 0.8922682404518127, 0.8945183753967285, 0.8932705521583557, 0.8930544853210449, 0.8929429054260254, 0.8936810493469238, 0.8938148617744446, 0.8921317458152771, 0.8940614461898804], \"mode\": \"lines+markers\", \"name\": \"lines+markers\"}], {\"title\": \"ROC AUC value by rand number seed\", \"xaxis\": {\"title\": \"Random number seed\"}, \"yaxis\": {\"title\": \"AUC\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace1 = go.Scatter(\n",
    "    y = cvData,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'lines+markers'\n",
    ")\n",
    "layout = dict(\n",
    "    title = 'ROC AUC value by rand number seed',\n",
    "    xaxis = dict(title = 'Random number seed'),\n",
    "    yaxis = dict(title = 'AUC')\n",
    ")\n",
    "py.iplot(dict(data=[trace1], layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 - Grid search with cross-validation using `GridSearchCV`\n",
    "\n",
    "#### TODO\n",
    "- Use `GridSearchCV` to do some simple parameter tuning\n",
    "\n",
    "\n",
    "#### Reference\n",
    "1. [Scikit Learn Doc - GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "2. [Example kernal 1 found on Kaggle](https://www.kaggle.com/garethjns/microsoft-lightgbm-with-parameter-tuning-0-823/code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
