{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Use plotly offline for fancy plots\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "# use cufflinks to bind plotly to pandas\n",
    "import cufflinks as cf \n",
    "from os import listdir\n",
    "# for display control\n",
    "from IPython.display import display\n",
    "# Gradient boosting using LightBGM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use LabelEncoder that seems to yield better result\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Garbage collection\n",
    "import gc\n",
    "gc.enable()\n",
    "# Lock pseudo-number seed\n",
    "randSeed = 1\n",
    "np.random.seed(randSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global verbose control\n",
    "PREVIEW_DATASET = 0\n",
    "ADD_STATS_FEATURES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to transform all catagorical fields using one hot ending\n",
    "def oneHotEncoding(df):\n",
    "    # Get list categorical features\n",
    "    catFeatures = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    # Convert to one hot encoding\n",
    "    ohe = pd.get_dummies(df, columns=catFeatures)\n",
    "    return ohe\n",
    "\n",
    "# Utility function to encode catagorical columns using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "def encodeLabel(field):\n",
    "    return le.fit_transform(field.astype(str))\n",
    "\n",
    "# Utility function to add descriptive statistics as secondary fields in the dataframe\n",
    "def addStatsFields(df, field):\n",
    "    df['MEAN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .mean()[field]\n",
    "    )\n",
    "    df['MEDIAN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .median()[field]\n",
    "    )\n",
    "    df['MAX_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .max()[field]\n",
    "    )\n",
    "    df['MIN_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .min()[field]\n",
    "    )\n",
    "    df['SUM_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .sum()[field]\n",
    "    )\n",
    "    df['VAR_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .var()[field]\n",
    "    )\n",
    "    df['CNT_' + field] = (\n",
    "        df[['SK_ID_CURR', field]]\n",
    "            .groupby('SK_ID_CURR')\n",
    "            .count()[field]\n",
    "    )\n",
    "    # Drop current field\n",
    "    df.drop(\n",
    "        [field], \n",
    "        axis = 1, \n",
    "        inplace = True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bureau_balance.csv.zip ...\n",
      "loading HomeCredit_columns_description.csv ...\n",
      "loading installments_payments.csv.zip ...\n",
      "loading previous_application.csv.zip ...\n",
      "loading application_test.csv.zip ...\n",
      "loading POS_CASH_balance.csv.zip ...\n",
      "loading credit_card_balance.csv.zip ...\n",
      "loading application_train.csv.zip ...\n",
      "loading bureau.csv.zip ...\n",
      "loading sample_submission.csv.zip ...\n",
      "Training dataset has 307511 samples, and 244 features\n",
      "Testing dataset has 48744 samples, and 244 features\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "dataPath = '/media/ephemeral0/data/home-credit-default-risk/'\n",
    "dataFiles = listdir(f\"{dataPath}\")\n",
    "for filename in dataFiles:\n",
    "    print(f'loading {filename} ...')\n",
    "    if '.csv.zip' in filename:\n",
    "        # compressed data file\n",
    "        locals()[filename.rstrip('.csv.zip')] = pd.read_csv(\n",
    "            f'{dataPath}/{filename}',\n",
    "            compression='zip', \n",
    "            header=0, \n",
    "            sep=',', \n",
    "            quotechar='\"'\n",
    "        )\n",
    "\n",
    "# Get output label and remove it from feature list\n",
    "dataTrain = application_train\n",
    "dataTest = application_test\n",
    "y = dataTrain['TARGET']\n",
    "\n",
    "# Transform using One Hot Encoding \n",
    "# (using only the training dataset features as reference)\n",
    "catFeatures = [\n",
    "    col \n",
    "    for col in dataTrain.columns \n",
    "    if dataTrain[col].dtype == 'object'\n",
    "]\n",
    "ohe = pd.concat([dataTrain,dataTest], sort=False)\n",
    "ohe = pd.get_dummies(ohe, columns = catFeatures)\n",
    "dataTrain = ohe.iloc[:dataTrain.shape[0],:]\n",
    "dataTest = ohe.iloc[dataTrain.shape[0]:,]\n",
    "del dataTrain['TARGET']\n",
    "del dataTest['TARGET']\n",
    "\n",
    "# Summarize dataset\n",
    "featureCnt = len(dataTrain.keys()) - 1\n",
    "numSamples = len(dataTrain)\n",
    "print(f'Training dataset has {numSamples} samples, and {featureCnt} features')\n",
    "featureCnt = len(dataTest.keys()) - 1\n",
    "numSamples = len(dataTest)\n",
    "print(f'Testing dataset has {numSamples} samples, and {featureCnt} features')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau datasets processed, 47 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preprocess bureau datasets\n",
    "if PREVIEW_DATASET:\n",
    "    print('Raw bureau_balance dataset')\n",
    "    display(bureau_balance.head(5))\n",
    "\n",
    "# Count by status\n",
    "bureauBalance = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .STATUS\n",
    "        .value_counts(normalize = False)\n",
    "        .unstack('STATUS')\n",
    ")\n",
    "# Rename columns to avoid conflict\n",
    "renameDict = {}\n",
    "for col in bureauBalance.columns:\n",
    "    renameDict[col] = 'STATUS_' + col\n",
    "bureauBalance.rename(columns = renameDict)\n",
    "\n",
    "# Add months balance data as new features\n",
    "bureauBalance['MONTHS_COUNT'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU') \n",
    "        .MONTHS_BALANCE          \n",
    "        .size()\n",
    ")\n",
    "bureauBalance['MONTHS_MAX'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .MONTHS_BALANCE\n",
    "        .max()\n",
    ")\n",
    "bureauBalance['MONTHS_MIN'] = (\n",
    "    bureau_balance\n",
    "        .groupby('SK_ID_BUREAU')\n",
    "        .MONTHS_BALANCE\n",
    "        .min()\n",
    ")\n",
    "if PREVIEW_DATASET:\n",
    "    print('Formatted')\n",
    "    display(bureauBalance.head(5))\n",
    "\n",
    "# Finally, merge the two bureau table together \n",
    "bureauData = bureau.join(bureauBalance, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "# Transform features\n",
    "bureauData = oneHotEncoding(bureauData).groupby('SK_ID_CURR').mean()\n",
    "bureauData['CNT_BURO'] = (\n",
    "    bureau[['SK_ID_BUREAU', 'SK_ID_CURR']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .count()['SK_ID_BUREAU']\n",
    ")\n",
    "del bureauData['SK_ID_BUREAU']\n",
    "if PREVIEW_DATASET:\n",
    "    print('Merged and transformed')\n",
    "    display(bureauData.head(5))\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = bureauData.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = bureauData.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'bureau datasets processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables and clean up memory\n",
    "del bureauBalance\n",
    "del bureauData\n",
    "del bureau_balance\n",
    "del bureau\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_application datasets processed, 163 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces previous_application\n",
    "# Transform with one hot encoding\n",
    "prevApplication = oneHotEncoding(previous_application)\n",
    "\n",
    "# Compute number of previous applications by counting SK_ID_PREV\n",
    "prevApplicationCnt = (\n",
    "    prevApplication[['SK_ID_CURR', 'SK_ID_PREV']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .count()\n",
    "        .SK_ID_PREV\n",
    ")\n",
    "# Group by mean\n",
    "prevApplication = prevApplication.groupby('SK_ID_CURR').mean()\n",
    "# Add back the number of previous applications\n",
    "prevApplication['CNT_PREV_APPLICATION'] = prevApplicationCnt\n",
    "# Remove the SK_ID_PREV feature because the average of it is meaningless\n",
    "del prevApplication['SK_ID_PREV']\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Merged dataset')\n",
    "    display(prevApplication.head(5))\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = prevApplication.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = prevApplication.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'previous_application datasets processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables and clean up memory\n",
    "del prevApplicationCnt\n",
    "del prevApplication\n",
    "del previous_application\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_CASH_balance dataset processed, 31 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces POS_CASH_balance\n",
    "# Encode using LabelEncoder\n",
    "posCashBal = POS_CASH_balance\n",
    "posCashBal.NAME_CONTRACT_STATUS = encodeLabel(POS_CASH_balance.NAME_CONTRACT_STATUS)\n",
    "posCashBal['CNT_UNIQUE_STATUS'] = (\n",
    "    posCashBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .nunique()\n",
    "        .NAME_CONTRACT_STATUS\n",
    ")\n",
    "posCashBal['MAX_UNIQUE_STATUS'] = (\n",
    "    posCashBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .max()\n",
    "        .NAME_CONTRACT_STATUS\n",
    ")\n",
    "\n",
    "# Add some secondary features\n",
    "if ADD_STATS_FEATURES:\n",
    "    posCashBal = addStatsFields(posCashBal, 'SK_DPD')\n",
    "    posCashBal = addStatsFields(posCashBal, 'SK_DPD_DEF')\n",
    "    posCashBal = addStatsFields(posCashBal, 'CNT_INSTALMENT_FUTURE')\n",
    "    posCashBal = addStatsFields(posCashBal, 'CNT_INSTALMENT')\n",
    "\n",
    "# Group by UserID\n",
    "#del posCashBal['SK_ID_PREV']\n",
    "posCashBal.drop(\n",
    "    ['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], \n",
    "    axis = 1, \n",
    "    inplace = True\n",
    ")\n",
    "posCashBal = posCashBal.groupby('SK_ID_CURR').mean()\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Processed dataset')\n",
    "    display(posCashBal.head())\n",
    "    \n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = posCashBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = posCashBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'POS_CASH_balance dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del posCashBal\n",
    "del POS_CASH_balance\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_card_balance dataset processed, 136 new features added\n"
     ]
    }
   ],
   "source": [
    "## Preproces credit_card_balance\n",
    "# Transform with one hot encoding\n",
    "# Rename fields that have already used in POS_CASH_balance\n",
    "creditCardBal = credit_card_balance.rename(\n",
    "    index = str, \n",
    "    columns = {\n",
    "        'NAME_CONTRACT_STATUS' : 'NAME_CONTRACT_STATUS_CREDIT',\n",
    "        'SK_DPD' : 'SK_DPD_CREDIT',\n",
    "        'SK_DPD_DEF' : 'SK_DPD_DEF_CREDIT'\n",
    "    }\n",
    ")\n",
    "#creditCardBal = oneHotEncoding(creditCardBal)\n",
    "creditCardBal.NAME_CONTRACT_STATUS_CREDIT = encodeLabel(\n",
    "    creditCardBal.NAME_CONTRACT_STATUS_CREDIT\n",
    ")\n",
    "creditCardBal['CNT_UNIQUE_STATUS_CREDIT'] = (\n",
    "    creditCardBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS_CREDIT']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .nunique()\n",
    "        .NAME_CONTRACT_STATUS_CREDIT\n",
    ")\n",
    "creditCardBal['MAX_UNIQUE_STATUS_CREDIT'] = (\n",
    "    creditCardBal[['SK_ID_CURR', 'NAME_CONTRACT_STATUS_CREDIT']]\n",
    "        .groupby('SK_ID_CURR')\n",
    "        .max()\n",
    "        .NAME_CONTRACT_STATUS_CREDIT\n",
    ")\n",
    "# Add some secondary features\n",
    "if ADD_STATS_FEATURES:\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_BALANCE')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_CREDIT_LIMIT_ACTUAL')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_ATM_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_OTHER_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_DRAWINGS_POS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_INST_MIN_REGULARITY')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_PAYMENT_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_PAYMENT_TOTAL_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_RECEIVABLE_PRINCIPAL')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_RECIVABLE')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'AMT_TOTAL_RECEIVABLE')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_ATM_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_OTHER_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_DRAWINGS_POS_CURRENT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'CNT_INSTALMENT_MATURE_CUM')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'SK_DPD_CREDIT')\n",
    "    creditCardBal = addStatsFields(creditCardBal, 'SK_DPD_DEF_CREDIT')\n",
    "\n",
    "# Group by ID\n",
    "creditCardBal.drop(\n",
    "    ['SK_ID_PREV', 'NAME_CONTRACT_STATUS_CREDIT'], \n",
    "    axis = 1, \n",
    "    inplace = True\n",
    ")\n",
    "creditCardBal = creditCardBal.groupby('SK_ID_CURR').mean()\n",
    "\n",
    "# Display merged dataset\n",
    "if PREVIEW_DATASET:\n",
    "    print('Processed dataset')\n",
    "    display(creditCardBal.head())\n",
    "    \n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = creditCardBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = creditCardBal.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'credit_card_balance dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del creditCardBal\n",
    "del credit_card_balance\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "# Add mean\n",
    "installPaymentMean = installments_payment.groupby('SK_ID_CURR').mean()\n",
    "del installPaymentMean['SK_ID_PREV']\n",
    "# Merge features into main training dataset\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = installPaymentMean.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = installPaymentMean.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "# Add min\n",
    "installPaymentMin = installments_payment.groupby('SK_ID_CURR').min()\n",
    "# Merge features into main training dataset\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = installPaymentMin.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = installPaymentMin.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "# Add max\n",
    "installPaymentMax = installments_payment.groupby('SK_ID_CURR').max()\n",
    "# Merge features into main training dataset\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = installPaymentMax.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = installPaymentMax.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'installments_payment dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del installPaymentMax\n",
    "del installPaymentMin\n",
    "del installPaymentMean\n",
    "del installments_payment\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installments_payment dataset processed, 30 new features added\n"
     ]
    }
   ],
   "source": [
    "colDictMean = {}\n",
    "colDictMin = {}\n",
    "colDictMax = {}\n",
    "colDictMedian = {}\n",
    "colDictVar = {}\n",
    "for col in installments_payment.columns:\n",
    "    if col not in ['SK_ID_CURR', 'SK_ID_PREV']:\n",
    "        colDictMean[col] = 'MEAN_' + col\n",
    "        colDictMin[col] = 'MIN_' + col\n",
    "        colDictMax[col] = 'MAX_' + col\n",
    "        colDictMedian[col] = 'MEDIAN_' + col\n",
    "        colDictVar[col] = 'VAR_' + col\n",
    "\n",
    "# Add mean\n",
    "installPayment = installments_payment.groupby('SK_ID_CURR').mean()\n",
    "del installPayment['SK_ID_PREV']\n",
    "installPayment = installPayment.rename(\n",
    "    columns = colDictMean\n",
    ")\n",
    "# Add min\n",
    "installPaymentMin = installments_payment.groupby('SK_ID_CURR').min()\n",
    "del installPaymentMin['SK_ID_PREV']\n",
    "installPaymentMin = installPaymentMin.rename(\n",
    "    columns = colDictMin\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMin,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMin\n",
    "# Add max\n",
    "installPaymentMax = installments_payment.groupby('SK_ID_CURR').max()\n",
    "del installPaymentMax['SK_ID_PREV']\n",
    "installPaymentMax = installPaymentMax.rename(\n",
    "    columns = colDictMax\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMax,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMax\n",
    "# Add median\n",
    "installPaymentMedian = installments_payment.groupby('SK_ID_CURR').median()\n",
    "del installPaymentMedian['SK_ID_PREV']\n",
    "installPaymentMedian = installPaymentMedian.rename(\n",
    "    columns = colDictMedian\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentMedian,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentMedian\n",
    "# Add variance\n",
    "installPaymentVar = installments_payment.groupby('SK_ID_CURR').var()\n",
    "del installPaymentVar['SK_ID_PREV']\n",
    "installPaymentVar = installPaymentVar.rename(\n",
    "    columns = colDictVar\n",
    ")\n",
    "installPayment = installPayment.merge(\n",
    "    right = installPaymentVar,\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "del installPaymentVar\n",
    "\n",
    "# Merge features into main training dataset\n",
    "featureCntBefore = len(dataTrain.keys()) - 1\n",
    "dataTrain = dataTrain.merge(\n",
    "    right = installPayment.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "dataTest = dataTest.merge(\n",
    "    right = installPayment.reset_index(),\n",
    "    how = 'left',\n",
    "    on = 'SK_ID_CURR'\n",
    ")\n",
    "featureCntAfter = len(dataTrain.keys()) - 1\n",
    "newFeatureCnt = featureCntAfter - featureCntBefore\n",
    "# Show stats\n",
    "print(f'installments_payment dataset processed, {newFeatureCnt} new features added')\n",
    "\n",
    "# Remove temporary variables\n",
    "del installPayment\n",
    "del installments_payment\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has 307511 samples, and 508 features\n",
      "Testing dataset has 48744 samples, and 508 features\n"
     ]
    }
   ],
   "source": [
    "## Final processing\n",
    "# Remove features too many missing values\n",
    "dataTestFinal  = dataTest[dataTest.columns[dataTrain.isnull().mean() < 0.85]]\n",
    "dataTrainFinal = dataTrain[dataTrain.columns[dataTrain.isnull().mean() < 0.85]]\n",
    "\n",
    "# Delete SK_ID_CURR field (not a feature) \n",
    "del dataTestFinal['SK_ID_CURR']\n",
    "del dataTrainFinal['SK_ID_CURR']\n",
    "\n",
    "# Summarize dataset\n",
    "featureCnt = len(dataTrainFinal.keys()) - 1\n",
    "numSamples = len(dataTrainFinal)\n",
    "print(f'Training dataset has {numSamples} samples, and {featureCnt} features')\n",
    "featureCnt = len(dataTestFinal.keys()) - 1\n",
    "numSamples = len(dataTestFinal)\n",
    "print(f'Testing dataset has {numSamples} samples, and {featureCnt} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignoredFields = ['TARGET']\n",
    "# Assemble into I/O dataset format\n",
    "#   X - All fields other than 'TARGET'\n",
    "#   Y - 'TARGET' fields\n",
    "#dataCols = dataTrain.columns\n",
    "#trainDataMask = [col for col in dataCols if col not in ignoredFields]\n",
    "#x = dataTrain[trainDataMask]\n",
    "\n",
    "# Split training data randomly using train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataTrainFinal, y, \n",
    "    test_size = 0.15,\n",
    "    random_state = randSeed,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# Create lgb dataset\n",
    "lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "lgb_test = lgb.Dataset(data=x_test, label=y_test)\n",
    "\n",
    "# Free up memory\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning and Performance References\n",
    "1. [LightGBM - Parameters](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst)\n",
    "2. [LightGBM - \n",
    "Parallel Learning Guide](https://github.com/Microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst)\n",
    "3. [LightGBM - Parameters Tuning Guide](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst)\n",
    "4. [Kaggle - GBM vs. XGBoost vs. LightGBM](https://www.kaggle.com/nschneider/gbm-vs-xgboost-vs-lightgbm)\n",
    "5. [Scikit Learn Doc - GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "### TODO\n",
    "- [ ] Consider use GridSearchCV to do some single parameter tuning\n",
    "- [ ] Check the performance of `'boosting_type': 'dart'`, which according to reference [3] improves accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[25]\tvalid_0's auc: 0.733438\n",
      "[50]\tvalid_0's auc: 0.746164\n",
      "[75]\tvalid_0's auc: 0.758495\n",
      "[100]\tvalid_0's auc: 0.767562\n",
      "[125]\tvalid_0's auc: 0.773771\n",
      "[150]\tvalid_0's auc: 0.778216\n",
      "[175]\tvalid_0's auc: 0.781326\n",
      "[200]\tvalid_0's auc: 0.783528\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'device' : 'cpu',\n",
    "    'nthread': 8,            # [CPU] number of OpenMP threads\n",
    "    'tree_learner' : 'feature',\n",
    "    'gpu_use_dp' : 'false',  # [GPU] set to 1 to enable 64bit float point\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'auc',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 10,\n",
    "    'learning_rate': 0.04,\n",
    "    'max_bin': 384,\n",
    "    'max_depth' : 12,\n",
    "    'min_split_gain': 0.5,\n",
    "    'min_child_weight': 1e-2,\n",
    "    'min_child_samples': 32,\n",
    "    'subsample_for_bin': 100000,\n",
    "    'subsample': 1,\n",
    "    'subsample_freq': 5,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round = 5000,\n",
    "    valid_sets = lgb_test,\n",
    "    early_stopping_rounds = 50,\n",
    "    verbose_eval = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and save to csv\n",
    "predResult = gbm.predict(dataTestFinal)\n",
    "submissionDataset = sample_submission\n",
    "submissionDataset.TARGET = predResult\n",
    "submissionDataset.to_csv('./lowpass_submission_v2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
